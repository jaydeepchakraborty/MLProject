{
 "cells": [
  {
   "cell_type": "raw",
   "id": "635d9587-092e-4d09-a4ca-551dbb88b7b4",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This file is for generating dataset ( data preperation ) and vocabulary\n",
    "1) read the cleaned data (csv)\n",
    "2) convert to torch dataset\n",
    "3) persisting torch dataset\n",
    "4) generate and persist vocabulary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70850d6-af2d-420f-a88e-02ac43d585fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install -U torch torchtext torchdata\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42537c02-a7d8-4ffa-8f78-8af8a2b32bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1d003c79-287b-43e9-a11a-c6578ca37df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf():\n",
    "    conf = {\n",
    "        \"path\": \"/Users/jaydeepchakraborty/JC/git-projects/model_util/\",\n",
    "        \"data\":{\n",
    "            \"train_data_path\": \"DataSets/NLPwithDisasterTweets/modf_train_data.csv\",\n",
    "            \"test_data_path\": \"DataSets/NLPwithDisasterTweets/modf_test_data.csv\",\n",
    "            \"vocab_path\": \"DataSets/NLPwithDisasterTweets/disaster_tweets.pt\",\n",
    "            \"train_dataset\": \"DataSets/NLPwithDisasterTweets/train_dataset.pt\",\n",
    "            \"validation_dataset\": \"DataSets/NLPwithDisasterTweets/validation_dataset.pt\",\n",
    "            \"test_dataset\": \"DataSets/NLPwithDisasterTweets/test_dataset.pt\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e57c4cb-1f19-4954-9e5b-2e7d5c9a978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetsDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, conf, ind=\"train\"):\n",
    "        self.conf = conf\n",
    "        if ind == \"train\":\n",
    "            self.data = pd.read_csv(self.conf['path'] + self.conf['data']['train_data_path'])\n",
    "            self.data = self.data.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string', \"target\": 'int64'})\n",
    "        if ind == \"test\":\n",
    "            self.data = pd.read_csv(self.conf['path'] + self.conf['data']['test_data_path'])\n",
    "            self.data = self.data.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string'})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d14944a4-6d0c-42af-abf1-03ca5a11d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetsDataSetHelper:\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        \n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        \n",
    "        self.conf = conf\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.vocab = None\n",
    "        self.train_dataset = None \n",
    "        self.validation_dataset = None\n",
    "        self.test_dataset = None   \n",
    "        \n",
    "    def load_data(self):\n",
    "        # FILE_PATH = conf['path'] + conf['data']['train_data_path']\n",
    "        # self.data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "        # self.data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "        # self.data_pipe = self.data_pipe.parse_csv(skip_lines=1, as_tuple=True)\n",
    "        \n",
    "        # loading entire data\n",
    "        self.dataset = DisasterTweetsDataSet(self.conf, ind=\"train\")\n",
    "        \n",
    "        # loading test data\n",
    "        self.test_dataset = DisasterTweetsDataSet(self.conf, ind=\"test\")\n",
    "        \n",
    "    def split_data(self):\n",
    "        # train, validation split\n",
    "        split_dataset = torch.utils.data.random_split(self.dataset, [0.8, 0.2])\n",
    "        \n",
    "        self.train_dataset, self.validation_dataset = split_dataset[0], split_dataset[1]\n",
    "        \n",
    "    def save_data(self, ind=\"train\"):\n",
    "        \n",
    "        if ind==\"train\" and self.train_dataset:\n",
    "            torch.save(self.train_dataset, self.conf['path'] + conf['data']['train_dataset'])       \n",
    "        elif ind==\"validation\" and self.validation_dataset:\n",
    "            torch.save(self.validation_dataset, self.conf['path'] + conf['data']['validation_dataset'])     \n",
    "        elif ind==\"test\" and self.test_dataset:\n",
    "            torch.save(self.test_dataset, self.conf['path'] + conf['data']['test_dataset'])\n",
    "            \n",
    "    def load_saved_data(self, ind=\"train\"):\n",
    "        if ind==\"train\":\n",
    "            self.train_dataset = torch.load(self.conf['path'] + conf['data']['train_dataset']) \n",
    "            print(f\"loaded train_dataset length {len(self.train_dataset)}\")\n",
    "        elif ind==\"validation\":\n",
    "            self.validation_dataset = torch.load(self.conf['path'] + conf['data']['validation_dataset'])\n",
    "            print(f\"loaded validation_dataset length {len(self.validation_dataset)}\")\n",
    "        elif ind==\"test\":\n",
    "            self.test_dataset = torch.load(self.conf['path'] + conf['data']['test_dataset'])\n",
    "            print(f\"loaded test_dataset length {len(self.test_dataset)}\")\n",
    "        \n",
    "        \n",
    "    def yield_tokens(self, data_iter):\n",
    "        for data_val in data_iter:\n",
    "            data_keyword, data_loc, data_text = str(data_val[1]), str(data_val[2]), str(data_val[3])\n",
    "            yield self.tokenizer(data_keyword + \" \" + data_loc + \" \" + data_text)\n",
    "    \n",
    "    def gen_vocab(self):\n",
    "        \n",
    "        self.vocab = build_vocab_from_iterator(self.yield_tokens(self.dataset),\n",
    "                                                min_freq=2,max_tokens=20000, \n",
    "                                                specials= ['<pad>', '<sos>', '<eos>', '<unk>'], \n",
    "                                                special_first=True\n",
    "                                            )\n",
    "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
    "        \n",
    "        print(\"Vocab generation:- \")\n",
    "        print(\"The length of the new vocab is\", len(self.vocab))    \n",
    "        new_stoi = self.vocab.get_stoi()\n",
    "        print(\"The index of 'new' is\", new_stoi['new'])\n",
    "        print(\"The index of '<pad>' is\", new_stoi['<pad>'])\n",
    "        print(\"The index of '<sos>' is\", new_stoi['<sos>'])\n",
    "        print(\"The index of '<eos>' is\", new_stoi['<eos>'])\n",
    "        print(\"The index of '<unk>' is\", new_stoi['<unk>'])\n",
    "        new_itos = self.vocab.get_itos()\n",
    "        print(\"The token at index 17 is\", new_itos[17])\n",
    "        \n",
    "    def save_vocab(self):\n",
    "        torch.save(self.vocab, self.conf['path'] + conf['data']['vocab_path'])\n",
    "        \n",
    "    def load_vocab(self):\n",
    "        self.vocab = torch.load(self.conf['path'] + conf['data']['vocab_path'])\n",
    "        print(\"Vocab loading:- \")\n",
    "        print(\"The length of the new vocab is\", len(self.vocab))    \n",
    "        new_stoi = self.vocab.get_stoi()\n",
    "        print(\"The index of 'new' is\", new_stoi['new'])\n",
    "        print(\"The index of '<pad>' is\", new_stoi['<pad>'])\n",
    "        print(\"The index of '<sos>' is\", new_stoi['<sos>'])\n",
    "        print(\"The index of '<eos>' is\", new_stoi['<eos>'])\n",
    "        print(\"The index of '<unk>' is\", new_stoi['<unk>'])\n",
    "        new_itos = self.vocab.get_itos()\n",
    "        print(\"The token at index 17 is\", new_itos[17])\n",
    "        \n",
    "            \n",
    "    def prnt_sample_data(self, ind=\"train\"):\n",
    "        if ind == \"train\":\n",
    "            print(\"sample train data\")\n",
    "            cnt = 0\n",
    "            for data in self.train_dataset:\n",
    "                print(data)\n",
    "                cnt += 1\n",
    "                if cnt == 3: break\n",
    "        if ind == \"validation\":\n",
    "            print(\"sample validation data\")\n",
    "            cnt = 0\n",
    "            for data in self.validation_dataset:\n",
    "                print(data)\n",
    "                cnt += 1\n",
    "                if cnt == 3: break         \n",
    "        if ind == \"test\":\n",
    "            print(\"sample test data\")\n",
    "            cnt = 0\n",
    "            for data in self.test_dataset:\n",
    "                print(data)\n",
    "                cnt += 1\n",
    "                if cnt == 3: break\n",
    "                \n",
    "    \n",
    "    def populate_disaster_tweets(self):\n",
    "        # loading the entire and test data \n",
    "        self.load_data()\n",
    "        \n",
    "        # spliting the data into train and validation\n",
    "        self.split_data()\n",
    "        \n",
    "        # saving the train dataset, validation dataset, test dataset\n",
    "        self.save_data(ind=\"train\")\n",
    "        self.save_data(ind=\"validation\")\n",
    "        self.save_data(ind=\"test\")\n",
    "        \n",
    "        # loading the saved dataset\n",
    "        self.load_saved_data(ind=\"train\")\n",
    "        self.load_saved_data(ind=\"validation\")\n",
    "        self.load_saved_data(ind=\"test\")\n",
    "        \n",
    "        # generating vocabulary\n",
    "        self.gen_vocab()\n",
    "        \n",
    "        # saving vocabulary\n",
    "        self.save_vocab()\n",
    "        \n",
    "        # loading the saved vocabulary\n",
    "        self.load_vocab()\n",
    "        \n",
    "        # printing sample data\n",
    "        self.prnt_sample_data(ind=\"train\")\n",
    "        self.prnt_sample_data(ind=\"validation\")\n",
    "        self.prnt_sample_data(ind=\"test\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "494bed12-f910-4c7d-b5cd-aae80a25e716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded train_dataset length 6019\n",
      "loaded validation_dataset length 1504\n",
      "loaded test_dataset length 3263\n",
      "Vocab generation:- \n",
      "The length of the new vocab is 1039\n",
      "The index of 'new' is 7\n",
      "The index of '<pad>' is 0\n",
      "The index of '<sos>' is 1\n",
      "The index of '<eos>' is 2\n",
      "The index of '<unk>' is 3\n",
      "The token at index 17 is via\n",
      "Vocab loading:- \n",
      "The length of the new vocab is 1039\n",
      "The index of 'new' is 7\n",
      "The index of '<pad>' is 0\n",
      "The index of '<sos>' is 1\n",
      "The index of '<eos>' is 2\n",
      "The index of '<unk>' is 3\n",
      "The token at index 17 is via\n",
      "sample train data\n",
      "id                    4823\n",
      "keyword         evacuation\n",
      "location              <NA>\n",
      "text        war evacuation\n",
      "target                   1\n",
      "Name: 3327, dtype: object\n",
      "id                               10241\n",
      "keyword                        volcano\n",
      "location                      northern\n",
      "text        trying blod volcano htptco\n",
      "target                               0\n",
      "Name: 7058, dtype: object\n",
      "id                                               2110\n",
      "keyword                                   catastrophe\n",
      "location                        welington new zealand\n",
      "text        photo catastrophe day trying get work get\n",
      "target                                              0\n",
      "Name: 1439, dtype: object\n",
      "sample validation data\n",
      "id                            134\n",
      "keyword                   acident\n",
      "location                     <NA>\n",
      "text        trafic acident injury\n",
      "target                          1\n",
      "Name: 87, dtype: object\n",
      "id                                          9888\n",
      "keyword                              traumatised\n",
      "location                              londonstan\n",
      "text        get wil one boy one went traumatised\n",
      "target                                         0\n",
      "Name: 6818, dtype: object\n",
      "id                                   7874\n",
      "keyword                       quarantined\n",
      "location                    heard mbikers\n",
      "text        home quarantined posible case\n",
      "target                                  1\n",
      "Name: 5453, dtype: object\n",
      "sample test data\n",
      "id                  0\n",
      "keyword          <NA>\n",
      "location         <NA>\n",
      "text        car crash\n",
      "Name: 0, dtype: object\n",
      "id                                            2\n",
      "keyword                                    <NA>\n",
      "location                                   <NA>\n",
      "text        heard earthquake city stay everyone\n",
      "Name: 1, dtype: object\n",
      "id                                               3\n",
      "keyword                                       <NA>\n",
      "location                                      <NA>\n",
      "text        forest fire spot gese acros stret save\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # configuration\n",
    "    conf = get_conf()\n",
    "    \n",
    "    disaster_tweets_obj = DisasterTweetsDataSetHelper(conf)\n",
    "    disaster_tweets_obj.populate_disaster_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c97cf22-33a4-4771-b8bb-5ffb16521907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed492022-ae45-4cdd-82e8-cb57ca7b298f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
