{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25f48f28-cf09-47ae-a438-5517d2a1f8c7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This file is for data cleaning ( data preperation )\n",
    "1) read the raw data (csv)\n",
    "2) modify/ clean the textual data\n",
    "3) persisting data (csv)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f3fd59b-64f3-4382-8713-c7e8ee0700a7",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Columns\n",
    "id - a unique identifier for each tweet\n",
    "text - the text of the tweet\n",
    "location - the location the tweet was sent from (may be blank)\n",
    "keyword - a particular keyword from the tweet (may be blank)\n",
    "target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20423dc3-9b2e-43dc-85f8-979127a1a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import statements\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import traceback\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70222973-d772-4f3d-b37b-eb770e85a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf():\n",
    "    conf = {\n",
    "        \"path\": \"/Users/jaydeepchakraborty/JC/git-projects/model_util/\",\n",
    "        \"data\":{\n",
    "            \"train_data_path\": \"DataSets/NLPwithDisasterTweets/train.csv\",\n",
    "            \"modf_train_data_path\": \"DataSets/NLPwithDisasterTweets/modf_train_data.csv\",\n",
    "            \"test_data_path\": \"DataSets/NLPwithDisasterTweets/test.csv\",\n",
    "            \"modf_test_data_path\": \"DataSets/NLPwithDisasterTweets/modf_test_data.csv\",\n",
    "            \"custom_stop_words\": \"DataSets/NLPwithDisasterTweets/custom_stop_word.txt\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f322d8-ed8b-4049-bab5-0a22336b41ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(conf, ind=\"train\"):\n",
    "    \n",
    "    if ind == \"train\":\n",
    "        df = pd.read_csv(conf[\"path\"] + conf[\"data\"][\"train_data_path\"])\n",
    "        df = df.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string', \"target\": 'int64'})\n",
    "    elif ind == \"test\":\n",
    "        df = pd.read_csv(conf[\"path\"] + conf[\"data\"][\"test_data_path\"])\n",
    "        df = df.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e54fb4d-5521-4850-83d3-235bfa49d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning utils\n",
    "\n",
    "## Removing Contractions: it's -> it is\n",
    "def decontracted(phrase):\n",
    "\n",
    "    # Specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # ..\n",
    "\n",
    "    # General\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    # ..\n",
    "\n",
    "    return phrase\n",
    "\n",
    "## Removing Punctuations: , . ! ? : ;..\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in list(string.punctuation): text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "## Removing Numbers\n",
    "def clean_number(text):\n",
    "    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n",
    "    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    return text\n",
    "\n",
    "## Removing Whitespaces\n",
    "def clean_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "## Removing Repeating Words\n",
    "def clean_repeat_words(text):\n",
    "    return re.sub(r\"(\\w*)(\\w)\\2(\\w*)\", r\"\\1\\2\\3\", text)\n",
    "\n",
    "## Removing custom words\n",
    "def clean_cust_word(phrase):\n",
    "    phrase = re.sub(r\"#\", \"\", phrase)\n",
    "    phrase = re.sub(r\"âˆš\", \"\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def clean_data_util(text):\n",
    "    text  = text.lower()\n",
    "    text = decontracted(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = clean_number(text)\n",
    "    text = clean_whitespace(text)\n",
    "    text = clean_repeat_words(text)\n",
    "    text = clean_cust_word(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def filter_data_util(word_tokens):\n",
    "    filterd_words = [w for w in word_tokens if len(w)>2]\n",
    "    return filterd_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e5765d-f528-40a6-8b4b-e2427bb74a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modf_func(row, final_stop_words, conf):\n",
    "    \n",
    "    # for Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # keyword column\n",
    "    word_txt =  row['keyword'] if isinstance(row['keyword'], str) else \"\"\n",
    "    word_txt = clean_data_util(word_txt)\n",
    "    word_tokens = word_tokenize(word_txt)\n",
    "    word_lemma = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    filtered_words = [w for w in word_lemma if not w in final_stop_words]\n",
    "    filtered_words = filter_data_util(filtered_words)\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    row['keyword'] = clean_text\n",
    "    \n",
    "    # location column\n",
    "    word_txt =  row['location'] if isinstance(row['location'], str) else \"\"\n",
    "    word_txt = clean_data_util(word_txt)\n",
    "    word_tokens = word_tokenize(word_txt)\n",
    "    word_lemma = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    filtered_words = [w for w in word_lemma if not w in final_stop_words]\n",
    "    filtered_words = filter_data_util(filtered_words)\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    row['location'] = clean_text\n",
    "    \n",
    "    # text column\n",
    "    word_txt =  row['text'] if isinstance(row['text'], str) else \"\"\n",
    "    word_txt = clean_data_util(word_txt)\n",
    "    word_tokens = word_tokenize(word_txt)\n",
    "    word_lemma = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    filtered_words = [w for w in word_lemma if not w in final_stop_words]\n",
    "    filtered_words = filter_data_util(filtered_words)\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "    row['text'] = clean_text\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec0d33a-242c-4f3f-a82e-e5a1b906c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "def load_custom_stop_words():\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words_path = conf['path'] + conf['data']['custom_stop_words']\n",
    "    \n",
    "    if os.path.isfile(custom_stop_words_path): \n",
    "        with open(custom_stop_words_path) as f:\n",
    "            custom_stop_words = f.read().splitlines()\n",
    "            custom_stop_words_set = set(custom_stop_words)\n",
    "            stop_words = stop_words.union(custom_stop_words_set)\n",
    "            \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721c8d5a-ae1c-4c15-8714-6cb2133d722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modf_data(df, conf, ind=\"train\"):\n",
    "    \n",
    "    final_stop_words = load_custom_stop_words()\n",
    "    \n",
    "    if ind == \"train\":\n",
    "        data_meta = {'id': 'int64', 'keyword': 'string', 'location': 'string', 'text': 'string', 'target': 'int64'}\n",
    "        ddf = dd.from_pandas(df, npartitions=6) # find your own number of partitions\n",
    "        ddf = ddf.apply(modf_func, meta=data_meta, args=(final_stop_words, conf,), axis=1).compute()\n",
    "\n",
    "        # df = df.apply(modf_func, axis=1)\n",
    "\n",
    "        # removing the rows where text-column are blank\n",
    "        ddf = ddf[ddf['text'] != '']\n",
    "    elif ind == \"test\":\n",
    "        data_meta = {'id': 'int64', 'keyword': 'string', 'location': 'string', 'text': 'string'}\n",
    "        ddf = dd.from_pandas(df, npartitions=6) # find your own number of partitions\n",
    "        ddf = ddf.apply(modf_func, meta=data_meta, args=(final_stop_words, conf,), axis=1).compute()\n",
    "        \n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc03de5-0391-42ba-80d3-f00cdd30e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, conf, ind=\"train\"):\n",
    "    if ind == \"train\":\n",
    "        df.to_csv(conf[\"path\"] + conf['data']['modf_train_data_path'], index=False,)\n",
    "    elif ind == \"test\":\n",
    "        df.to_csv(conf[\"path\"] + conf['data']['modf_test_data_path'], index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b833189-68d0-49c0-bf2a-555925f2fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.22 s, sys: 161 ms, total: 9.38 s\n",
      "Wall time: 9.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        conf = get_conf()\n",
    "        \n",
    "        # modifying the training data\n",
    "        train_data_df = read_data(conf, ind=\"train\")\n",
    "        train_modf_df = modf_data(train_data_df, conf, ind=\"train\")\n",
    "        save_data(train_modf_df, conf, ind=\"train\")\n",
    "        \n",
    "        # modifying the testing data\n",
    "        test_data_df = read_data(conf, ind=\"test\")\n",
    "        test_modf_df = modf_data(test_data_df, conf, ind=\"test\")\n",
    "        save_data(test_modf_df, conf, ind=\"test\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fda00-0cdf-4d40-8695-7689c8926f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
