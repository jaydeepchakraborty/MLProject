{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dea46e-1f3f-4a20-9751-1d776d2e5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file is for training model and save final output\n",
    "1) read the processed files and vocabulary\n",
    "2) train model\n",
    "3) model inference\n",
    "4) generate and persist submission csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f3cd51-49f8-4f73-8e1c-00e7e09b0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a7ecbdb-98dc-455a-8c96-485011c1fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf():\n",
    "    conf = {\n",
    "            \"path\": \"/Users/jaydeepchakraborty/JC/git-projects/model_util/\",\n",
    "            \"data\":{\n",
    "                    \"vocab_path\": \"DataSets/NLPwithDisasterTweets/disaster_tweets.pt\",\n",
    "                    \"train_dataset\": \"DataSets/NLPwithDisasterTweets/train_dataset.pt\",\n",
    "                    \"validation_dataset\": \"DataSets/NLPwithDisasterTweets/validation_dataset.pt\",\n",
    "                    \"test_dataset\": \"DataSets/NLPwithDisasterTweets/test_dataset.pt\",\n",
    "            },\n",
    "            \"model\":{\n",
    "                \"model_path\": \"Models/NLPwithDisasterTweets/disaster_tweet_\",\n",
    "                \"train_batch_size\": 5,\n",
    "                \"validation_batch_size\": 5,\n",
    "                \"test_batch_size\": 1,\n",
    "                \"n_epoch\": 100,\n",
    "                \"valid_epoch\": 10\n",
    "            },\n",
    "            \"op\":{\n",
    "                \"op_path\": \"DataSets/NLPwithDisasterTweets/submission.csv\"\n",
    "            }\n",
    "    }\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adced406-6d35-45c9-aeee-fef343008dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetsDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, conf, ind=\"train\"):\n",
    "        self.conf = conf\n",
    "        if ind == \"train\":\n",
    "            self.data = pd.read_csv(self.conf['path'] + self.conf['data']['train_data_path'])\n",
    "            self.data = self.data.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string', \"target\": 'int64'})\n",
    "        if ind == \"test\":\n",
    "            self.data = pd.read_csv(self.conf['path'] + self.conf['data']['test_data_path'])\n",
    "            self.data = self.data.astype({\"id\": 'int64', \"keyword\": 'string', \"location\": 'string', \"text\": 'string'})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b53ecc03-9e05-4134-a15c-a0e75704e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetModel(nn.Module):\n",
    "    def __init__(self, vocab_size, **kwargs):\n",
    "        #Constructor\n",
    "        super(DisasterTweetModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # variables\n",
    "        self.embedding_dim = 300\n",
    "        self.hidden_dim = 100\n",
    "        self.num_layers = 1\n",
    "        self.bidirectional = True\n",
    "        self.batch_first = True\n",
    "        self.output_dim = 1\n",
    "\n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "\n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                            hidden_size=self.hidden_dim, \n",
    "                            num_layers=self.num_layers, \n",
    "                            bidirectional=self.bidirectional,\n",
    "                            batch_first=self.batch_first)\n",
    "\n",
    "        #dense layer / linear layer\n",
    "        self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
    "\n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, txt):\n",
    "\n",
    "        '''\n",
    "        Step 1: pass through the embedding layer to convert text into vectors\n",
    "        '''\n",
    "        # embed_txt ~ [batch_size, seq_len, embedding_dim] \n",
    "        embed_txt = self.embedding(txt)\n",
    "\n",
    "        '''\n",
    "        Step 2: passing the embeddings through LSTM layer\n",
    "        '''\n",
    "        # lstm_out ~ [batch_size, seq_len, (2 * hidden_dim)] \n",
    "        lstm_out, (h_n, c_n) = self.lstm(embed_txt)\n",
    "\n",
    "        '''\n",
    "        Step 3: sum all the hidden states\n",
    "        '''\n",
    "        # lstm_out ~ [include dimention, remove dimention, include dimention] \n",
    "        # concat_out ~ [batch_size, (2 * hidden_dim)] #concatenate hidden states\n",
    "        # concat_out = lstm_out[ : , -1, : ]  #concatenate hidden states\n",
    "        sum_ip = lstm_out.sum(dim=1)  #summing up hidden states\n",
    "        # avg_ip = lstm_out.mean(dim=1)  #averaging the hidden states\n",
    "\n",
    "        '''\n",
    "        Step 4: feeding the weighted value to a linear layer\n",
    "        '''\n",
    "        # fc_out ~ [batch_size, output_dim]\n",
    "        fc_out = self.fc(sum_ip)\n",
    "\n",
    "        '''\n",
    "        Step 5: feeding the linear output to activation function\n",
    "        '''\n",
    "        # out ~ [batch_size, output_dim]\n",
    "        out = self.act(fc_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09bbd9c4-803d-482d-a283-091a6a6db8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetModelHelper:\n",
    "    \n",
    "    def __init__(self, conf):\n",
    "        self.conf = conf\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.train_dataset = None\n",
    "        self.validation_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.vocab = None\n",
    "    \n",
    "    # loading the data\n",
    "    def load_data(self):\n",
    "        self.train_dataset = torch.load(self.conf['path'] + conf['data']['train_dataset']) \n",
    "        self.validation_dataset = torch.load(self.conf['path'] + conf['data']['validation_dataset'])\n",
    "        self.test_dataset = torch.load(self.conf['path'] + conf['data']['test_dataset'])\n",
    "        self.vocab = torch.load(self.conf['path'] + conf['data']['vocab_path'])\n",
    "        \n",
    "        # pretrained FastText vector \n",
    "        pretrained_vectors = torchtext.vocab.FastText()\n",
    "        self.pretrained_embedding = pretrained_vectors.get_vecs_by_tokens(self.vocab.get_itos())\n",
    "        \n",
    "        return\n",
    "\n",
    "    # for training and validation\n",
    "    def collate_batch(self, batch):\n",
    "        text_transform = lambda x: [self.vocab['<sos>']] + [self.vocab[token] for token in x.split(\" \")] + [self.vocab['<eos>']]\n",
    "        label_transform = lambda x: 1.0 if x == 1 else 0.0\n",
    "        \n",
    "        keyword_lst, location_lst, text_lst, trgt_lst = [], [], [], []\n",
    "\n",
    "        for _id, _keyword, _location, _text, _trgt in batch:\n",
    "            \n",
    "            processed_keyword = torch.tensor(text_transform(_keyword if isinstance(_keyword, str) else ''))\n",
    "            keyword_lst.append(processed_keyword)\n",
    "            \n",
    "            processed_location = torch.tensor(text_transform(_location if isinstance(_location, str) else ''))\n",
    "            location_lst.append(processed_location)\n",
    "            \n",
    "            processed_text = torch.tensor(text_transform(_text if isinstance(_text, str) else ''))\n",
    "            text_lst.append(processed_text)\n",
    "            \n",
    "            trgt_lst.append(label_transform(_trgt))\n",
    "\n",
    "        \n",
    "        return_keyword_lst = pad_sequence(keyword_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        return_location_lst = pad_sequence(location_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        return_text_lst = pad_sequence(text_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        return_trgt_lst = torch.tensor(trgt_lst)\n",
    "        \n",
    "        return return_keyword_lst, return_location_lst,  return_text_lst, return_trgt_lst\n",
    "    \n",
    "    # for testing\n",
    "    def collate_batch_test(self, batch):\n",
    "        text_transform = lambda x: [self.vocab['<sos>']] + [self.vocab[token] for token in x.split(\" \")] + [self.vocab['<eos>']]\n",
    "        label_transform = lambda x: 1.0 if x == 1 else 0.0\n",
    "        \n",
    "        id_lst, keyword_lst, location_lst, text_lst = [], [], [], []\n",
    "\n",
    "        for _id, _keyword, _location, _text in batch:\n",
    "            \n",
    "            id_lst.append(_id) # needed for submission\n",
    "            \n",
    "            processed_keyword = torch.tensor(text_transform(_keyword if isinstance(_keyword, str) else ''))\n",
    "            keyword_lst.append(processed_keyword)\n",
    "            \n",
    "            processed_location = torch.tensor(text_transform(_location if isinstance(_location, str) else ''))\n",
    "            location_lst.append(processed_location)\n",
    "            \n",
    "            processed_text = torch.tensor(text_transform(_text if isinstance(_text, str) else ''))\n",
    "            text_lst.append(processed_text)\n",
    "\n",
    "        \n",
    "        return_id_lst = torch.tensor(id_lst)\n",
    "        return_keyword_lst = pad_sequence(keyword_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        return_location_lst = pad_sequence(location_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        return_text_lst = pad_sequence(text_lst, padding_value=3.0, batch_first=True) # 3 is for <unk>\n",
    "        \n",
    "        return return_id_lst, return_keyword_lst, return_location_lst,  return_text_lst\n",
    "    \n",
    "    def gen_loader(self):\n",
    "        self.train_dataloader = DataLoader(self.train_dataset,\n",
    "                                           batch_size = self.conf['model']['train_batch_size'],\n",
    "                                           collate_fn=self.collate_batch)\n",
    "        \n",
    "        self.validation_dataloader = DataLoader(self.validation_dataset,\n",
    "                                   batch_size = self.conf['model']['validation_batch_size'],\n",
    "                                   collate_fn=self.collate_batch)\n",
    "        \n",
    "        self.test_dataloader = DataLoader(self.test_dataset,\n",
    "                                   batch_size = self.conf['model']['test_batch_size'],\n",
    "                                   collate_fn=self.collate_batch_test)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    #define metric\n",
    "    def binary_accuracy(self, preds, y):\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(preds)\n",
    "        correct = (rounded_preds == y).float() \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "    \n",
    "    # training model\n",
    "    def train(self):\n",
    "        #instantiate the model\n",
    "        train_model = DisasterTweetModel(len(self.vocab))\n",
    "        # assigning pretrained_embedding \n",
    "        train_model.embedding.weight.data = self.pretrained_embedding\n",
    "        train_model = train_model.to(self.device)\n",
    "        \n",
    "        \n",
    "        #define the optimizer\n",
    "        optimizer = optim.Adam(train_model.parameters())\n",
    "\n",
    "        #define the loss\n",
    "        criterion = nn.BCELoss()\n",
    "        criterion = criterion.to(self.device)\n",
    "\n",
    "        #set the model in training phase\n",
    "        train_model.train()\n",
    "        \n",
    "        N_EPOCHS = self.conf['model']['n_epoch']\n",
    "        VALIDATION_EPOCH = self.conf['model']['valid_epoch']\n",
    "\n",
    "        for epoch in range(N_EPOCHS+1):\n",
    "            #initialize every epoch \n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "\n",
    "            for idx, (_keyword, _location, _text, _trgt) in enumerate(self.train_dataloader):\n",
    "                #resets the gradients after every batch\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #get prediction\n",
    "                predictions = train_model(_text)\n",
    "                preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "                \n",
    "                #compute the loss\n",
    "                loss = criterion(preds, _trgt)\n",
    "\n",
    "                #compute the binary accuracy\n",
    "                acc = self.binary_accuracy(preds, _trgt)   \n",
    "\n",
    "                #backpropage the loss and compute the gradients\n",
    "                loss.backward()\n",
    "\n",
    "                #update the weights\n",
    "                optimizer.step() \n",
    "\n",
    "                # compute loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "            if epoch%VALIDATION_EPOCH == 0:\n",
    "                train_model.eval() # set the model in eval phase\n",
    "                valid_epoc_loss, valid_epoch_acc = self.test(train_model, criterion)\n",
    "                train_model.train() # return back to training phase\n",
    "\n",
    "                print(\"epoch:- \",epoch)\n",
    "                print(\"training===> \",\"loss:- \", epoch_loss / len(self.train_dataloader), \"  accuracy:- \", epoch_acc / len(self.train_dataloader))\n",
    "                print(\"validation===> \",\"loss:- \", valid_epoc_loss, \"  accuracy:- \", valid_epoch_acc)\n",
    "                \n",
    "                # saving intermediate model\n",
    "                torch.save(train_model.state_dict(), conf['path']+conf['model']['model_path']+str(epoch)+'.pt')\n",
    "\n",
    "            if epoch == N_EPOCHS-1:\n",
    "                torch.save(train_model.state_dict(), conf['path']+conf['model']['model_path']+'final'+'.pt')\n",
    "                    \n",
    "        return\n",
    "\n",
    "    \n",
    "    # validation\n",
    "    def test(self, train_model, criterion):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx, (_keyword, _location, _text, _trgt) in enumerate(self.validation_dataloader):\n",
    "                #get prediction\n",
    "                predictions = train_model(_text)\n",
    "                preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "                \n",
    "                #compute the loss\n",
    "                loss = criterion(preds, _trgt)\n",
    "                \n",
    "                #compute the binary accuracy\n",
    "                acc = self.binary_accuracy(preds, _trgt)\n",
    "                \n",
    "                # compute loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "                \n",
    "        valid_epoc_loss = epoch_loss / len(self.validation_dataloader)\n",
    "        valid_epoch_acc = epoch_acc / len(self.validation_dataloader)\n",
    "\n",
    "        return valid_epoc_loss, valid_epoch_acc\n",
    "    \n",
    "    def inference(self):\n",
    "        \n",
    "        #instantiate the model\n",
    "        inf_model = DisasterTweetModel(len(self.vocab))\n",
    "        inf_model = inf_model.to(self.device)\n",
    "        \n",
    "        #loading the model\n",
    "        model_path = conf['path'] + conf['model']['model_path'] + 'final' + '.pt'\n",
    "        inf_model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        inf_model.eval() # set the model in eval phase\n",
    "        \n",
    "        result_lst = []\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for idx, (_id, _keyword, _location, _text) in enumerate(self.test_dataloader):\n",
    "                predictions = inf_model(_text)\n",
    "                result_lst.append([int(_id.item()), int(predictions.item())])\n",
    "                \n",
    "        df = pd.DataFrame(result_lst, columns =['id', 'target'], dtype = 'int64')\n",
    "        \n",
    "        df.to_csv(conf['path']+conf['op']['op_path'], index=False,)\n",
    "            \n",
    "        \n",
    "    def perform(self):\n",
    "        self.load_data()\n",
    "        self.gen_loader()\n",
    "        # self.train()\n",
    "        self.inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f41c83c8-49cd-4430-8521-4d37a5c90ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.27 s, sys: 3.45 s, total: 11.7 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    conf = get_conf()\n",
    "    disaster_tweets_model_obj = DisasterTweetModelHelper(conf)\n",
    "    disaster_tweets_model_obj.perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674d3cb-1219-4881-9fe6-6d0a95b67012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
