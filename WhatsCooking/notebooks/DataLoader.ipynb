{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4582990f-8b2c-4de1-830d-7c90abef05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting os environement\n",
    "import os\n",
    "os.environ[\"ENV_NM\"] = \"dev\"\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eefa0270-3ea2-4249-8658-05dbf2613bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from util.util_lib import *\n",
    "import util.util_cnst as cnst\n",
    "from CookingDataset import CookingDataset \n",
    "from CookingBatchSampler import CookingBatchSampler\n",
    "from CookingCollator import CookingCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceba4849-e4f1-46a0-b3a4-d9bea6408ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config file\n",
    "def get_conf(): \n",
    "    conf = EnvYAML(cnst.CONFIG_FL)  \n",
    "    conf = conf[conf['env_nm']]\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e8948b-b43b-41f0-a480-6daf20e445d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(conf):\n",
    "    print(f\"vocab read STARTED:- {conf['vocab']}\")\n",
    "    vocab = torch.load(conf['vocab'])\n",
    "    print(f\"vocab read FINISHED:- {conf['vocab']}\")\n",
    "\n",
    "    print(\"unk:- \", vocab.get_stoi()[\"<UNK>\"])\n",
    "    print(\"pad:- \", vocab.get_stoi()[\"<PAD>\"])\n",
    "    # print(\"sos:- \", vocab.get_stoi()[\"<SOS>\"]) #not present in dictionary\n",
    "    print(\"The first word in vocab is \", vocab.get_itos()[0])\n",
    "    print(\"The second word in vocab is \", vocab.get_itos()[1])\n",
    "    print(\"The third word in vocab is \", vocab.get_itos()[2])\n",
    "    print(\"The last word in vocab is \", vocab.get_itos()[len(vocab)-1])\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9985c6d9-3ac9-432d-888a-c4fb241faf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder to a file\n",
    "def load_label(conf):\n",
    "    print(f\"lebel encode load STARTED:- {conf['lbl_enc']}\")\n",
    "    with open(conf['lbl_enc'], 'rb') as file:\n",
    "        le = pickle.load(file)\n",
    "    print(f\"lebel encode load FINISHED:- {conf['lbl_enc']}\")\n",
    "    \n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a716a63c-0852-4d48-88aa-ea8faf4ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataloader(data_set, le, vocab, ind, conf):\n",
    "    \n",
    "    data_sampler = CookingBatchSampler(data_set, ind, conf)\n",
    "    \n",
    "    if ind == \"train\":\n",
    "        cookingCollator = CookingCollator(vocab, le, \"train\", conf)\n",
    "        bucket_dataloader = DataLoader(data_set, \n",
    "                                       batch_sampler=data_sampler,\n",
    "                                       collate_fn=cookingCollator)\n",
    "    elif ind == \"valid\":\n",
    "        cookingCollator = CookingCollator(vocab, le, \"valid\", conf)\n",
    "        bucket_dataloader = DataLoader(data_set, \n",
    "                                       batch_sampler=data_sampler,\n",
    "                                       collate_fn=cookingCollator)\n",
    "    elif ind == \"test\":\n",
    "        cookingCollator = CookingCollator(vocab, le, \"test\", conf)\n",
    "        bucket_dataloader = DataLoader(data_set, \n",
    "                                       batch_sampler=data_sampler,\n",
    "                                       collate_fn=cookingCollator)\n",
    "    \n",
    "    return bucket_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95565f6d-109d-4ef9-8f65-c4d9f7c4df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataloader(dataloader_obj, path):\n",
    "    print(f\"torch dataloader STARTED:- {path}\")\n",
    "    torch.save(dataloader_obj, path)  \n",
    "    print(f\"torch dataloader FINISHED:- {path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0fd2e6-9cd5-4f3a-9a9f-a5c6b9de4f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab read STARTED:- data/vocab/vocab_obj.pth\n",
      "vocab read FINISHED:- data/vocab/vocab_obj.pth\n",
      "unk:-  0\n",
      "pad:-  1\n",
      "The first word in vocab is  <UNK>\n",
      "The second word in vocab is  <PAD>\n",
      "The third word in vocab is  pepper\n",
      "The last word in vocab is  zero\n",
      "lebel encode load STARTED:- data/encoder/label_encoder.pkl\n",
      "lebel encode load FINISHED:- data/encoder/label_encoder.pkl\n",
      "data loaded STARTED:- data/train/train_preprocess.csv\n",
      "data loaded FINISHED:- data/train/train_preprocess.csv\n",
      "data_df shape:- (31819, 4)\n",
      "Train Data =====> \n",
      "{'text_list': [['raspberries', 'sugar'], ['chicken', 'chicken breasts'], ['sour cream', 'heavy cream'], ['longgrain rice', 'cold water'], ['coconut', 'water'], ['coarse salt', 'polenta', 'extravirgin olive oil'], ['pepper', 'fresh chevre', 'asiago'], ['coarse salt', 'water', 'pork shoulder butt']], 'text_to_vocab_list': tensor([[[ 457,    0,    0],\n",
      "         [   9,    0,    0],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[  12,    0,    0],\n",
      "         [  12,   88,    0],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[  97,   28,    0],\n",
      "         [ 125,   28,    0],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[ 292,   29,    0],\n",
      "         [ 226,   15,    0],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[  82,    0,    0],\n",
      "         [  15,    0,    0],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[ 183,    3,    0],\n",
      "         [ 488,    0,    0],\n",
      "         [  66,   13,    4]],\n",
      "\n",
      "        [[   2,    0,    0],\n",
      "         [   7, 1248,    0],\n",
      "         [ 638,    0,    0]],\n",
      "\n",
      "        [[ 183,    3,    0],\n",
      "         [  15,    0,    0],\n",
      "         [  73,  265,  526]]]), 'label_list': ['french', 'indian', 'french', 'chinese', 'indian', 'italian', 'spanish', 'mexican'], 'label_to_encode_list': tensor([ 5,  7,  5,  3,  7,  9, 17, 13])}\n",
      "torch dataloader STARTED:- data/train/train_dataloader.pt\n",
      "torch dataloader FINISHED:- data/train/train_dataloader.pt\n",
      "data loaded STARTED:- data/valid/valid_preprocess.csv\n",
      "data loaded FINISHED:- data/valid/valid_preprocess.csv\n",
      "data_df shape:- (7955, 4)\n",
      "Valid Data =====> \n",
      "{'text_list': [['whipping cream', 'chocolate baking bar'], ['salt', 'edamame', 'seasoning salt'], ['blackberries', 'chambord', 'champagne'], ['powdered sugar', 'butter', 'peaches'], ['water', 'masa harina', 'salt'], ['mayonaise', 'hot sauce', 'lime juice'], ['lime juice', 'sour cream', 'lime zest'], ['vegetable oil', 'ground cumin', 'boneless chicken breast']], 'text_to_vocab_list': tensor([[[ 166,   28,    0],\n",
      "         [ 164,   58, 1527],\n",
      "         [   0,    0,    0]],\n",
      "\n",
      "        [[   3,    0,    0],\n",
      "         [ 649,    0,    0],\n",
      "         [  79,    3,    0]],\n",
      "\n",
      "        [[ 603,    0,    0],\n",
      "         [1538,    0,    0],\n",
      "         [ 835,    0,    0]],\n",
      "\n",
      "        [[ 228,    9,    0],\n",
      "         [  18,    0,    0],\n",
      "         [ 343,    0,    0]],\n",
      "\n",
      "        [[  15,    0,    0],\n",
      "         [ 504,  597,    0],\n",
      "         [   3,    0,    0]],\n",
      "\n",
      "        [[ 177,    0,    0],\n",
      "         [ 101,    8,    0],\n",
      "         [  40,   24,    0]],\n",
      "\n",
      "        [[  40,   24,    0],\n",
      "         [  97,   28,    0],\n",
      "         [  40,  163,    0]],\n",
      "\n",
      "        [[  33,    4,    0],\n",
      "         [   6,   43,    0],\n",
      "         [  65,   12,  138]]]), 'label_list': ['french', 'japanese', 'french', 'southern_us', 'mexican', 'japanese', 'mexican', 'mexican'], 'label_to_encode_list': tensor([ 5, 11,  5, 16, 13, 11, 13, 13])}\n",
      "torch dataloader STARTED:- data/valid/valid_dataloader.pt\n",
      "torch dataloader FINISHED:- data/valid/valid_dataloader.pt\n",
      "data loaded STARTED:- data/test/test_preprocess.csv\n",
      "data loaded FINISHED:- data/test/test_preprocess.csv\n",
      "data_df shape:- (9944, 3)\n",
      "Test Data =====> \n",
      "{'text_list': [['lager beer', 'beer'], ['beaujolais', 'cr√®me de cassis'], ['konbu', 'bonito flakes'], ['crema mexican', 'sauce', 'chipotle chile'], ['water', 'beef ribs', 'sea salt'], ['rice flour', 'candy', 'walnuts'], ['polenta', 'chicken broth', 'cream cheese'], ['salsa', 'taco seasoning', 'boneless skinless chicken breasts']], 'text_to_vocab_list': tensor([[[1015,  327,    0,    0],\n",
      "         [ 327,    0,    0,    0],\n",
      "         [   0,    0,    0,    0]],\n",
      "\n",
      "        [[   0,    0,    0,    0],\n",
      "         [ 478,  386, 1802,    0],\n",
      "         [   0,    0,    0,    0]],\n",
      "\n",
      "        [[ 760,    0,    0,    0],\n",
      "         [ 631,   98,    0,    0],\n",
      "         [   0,    0,    0,    0]],\n",
      "\n",
      "        [[ 645,  243,    0,    0],\n",
      "         [   8,    0,    0,    0],\n",
      "         [ 299,   84,    0,    0]],\n",
      "\n",
      "        [[  15,    0,    0,    0],\n",
      "         [  55,  189,    0,    0],\n",
      "         [ 118,    3,    0,    0]],\n",
      "\n",
      "        [[  29,   17,    0,    0],\n",
      "         [ 795,    0,    0,    0],\n",
      "         [ 345,    0,    0,    0]],\n",
      "\n",
      "        [[ 488,    0,    0,    0],\n",
      "         [  12,   44,    0,    0],\n",
      "         [  28,   11,    0,    0]],\n",
      "\n",
      "        [[ 130,    0,    0,    0],\n",
      "         [ 205,   79,    0,    0],\n",
      "         [  65,   78,   12,   88]]])}\n",
      "torch dataloader STARTED:- data/test/test_dataloader.pt\n",
      "torch dataloader FINISHED:- data/test/test_dataloader.pt\n",
      "CPU times: user 1min 41s, sys: 872 ms, total: 1min 42s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # reading configuration\n",
    "        conf = get_conf()\n",
    "        \n",
    "        # loading the vocab object\n",
    "        vocab = read_vocab(conf)\n",
    "        \n",
    "        # loading the label encoder\n",
    "        le = load_label(conf)\n",
    "        \n",
    "        # ---------------- TRAIN ---------------- #\n",
    "        # converting torch Dataset\n",
    "        train_dataset = CookingDataset(\"train\", conf)\n",
    "        \n",
    "        # converting to torch dataloader ( train )\n",
    "        train_dataloader = gen_dataloader(train_dataset, le, vocab, \"train\", conf)\n",
    "        \n",
    "        print(f\"Train Data =====> \")\n",
    "        print(next(iter(train_dataloader)))\n",
    "        \n",
    "        # saving dataloader ( train )\n",
    "        dataloader_path = conf['data']['data_fl_path'] + conf['data']['train_dataloader']\n",
    "        save_dataloader(train_dataloader, dataloader_path)\n",
    "        \n",
    "        \n",
    "        # ---------------- VALID ---------------- #\n",
    "        # converting torch Dataset\n",
    "        valid_dataset = CookingDataset(\"valid\", conf)\n",
    "        \n",
    "        # converting to torch dataloader ( valid )\n",
    "        valid_dataloader = gen_dataloader(valid_dataset, le, vocab, \"valid\", conf)\n",
    "        \n",
    "        print(f\"Valid Data =====> \")\n",
    "        print(next(iter(valid_dataloader)))\n",
    "        \n",
    "        # saving dataloader ( valid )\n",
    "        dataloader_path = conf['data']['data_fl_path'] + conf['data']['valid_dataloader']\n",
    "        save_dataloader(valid_dataloader, dataloader_path)\n",
    "        \n",
    "        \n",
    "        # ---------------- TEST ---------------- #\n",
    "        # converting torch Dataset\n",
    "        test_dataset = CookingDataset(\"test\", conf)\n",
    "        \n",
    "        # converting to torch dataloader ( test )\n",
    "        test_dataloader = gen_dataloader(test_dataset, le, vocab, \"test\", conf)\n",
    "        \n",
    "        print(f\"Test Data =====> \")\n",
    "        print(next(iter(test_dataloader)))\n",
    "        \n",
    "        # saving dataloader ( test )\n",
    "        dataloader_path = conf['data']['data_fl_path'] + conf['data']['test_dataloader']\n",
    "        save_dataloader(test_dataloader, dataloader_path)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa25ad-ed61-444a-92d4-66583bf72c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c2c1bc-1b69-4292-a8e5-c142ab98a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hussainwali.medium.com/transforming-your-text-data-with-pytorch-12ec1b1c9ae6\n",
    "# https://hussainwali.medium.com/using-fasttext-embeddings-in-pytorch-boosting-neural-network-performance-fe017c39c7c3\n",
    "# https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb\n",
    "# https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71\n",
    "# https://medium.com/geekculture/pytorch-datasets-dataloader-samplers-and-the-collat-fn-bbfc7c527cf1\n",
    "# https://discuss.pytorch.org/t/batching-tensor-with-different-size/133147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979bd00d-64e9-4db1-ab10-525eabb75843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e9bec-c7d5-45ff-bd22-9f348c1b6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "  def __init__(self, vocab_size, **kwargs):\n",
    "    #Constructor\n",
    "    super(Classifier, self).__init__(**kwargs)\n",
    "\n",
    "    # variables\n",
    "    self.embedding_dim = 2\n",
    "    self.hidden_dim = 32\n",
    "    self.num_layers = 1\n",
    "    self.bidirectional = True\n",
    "    self.batch_first = True\n",
    "    self.output_dim = 1\n",
    "\n",
    "    #embedding layer\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                  embedding_dim=self.embedding_dim, \n",
    "                                  padding_idx=0)\n",
    "\n",
    "    \n",
    "    #lstm layer\n",
    "    self.lstm = nn.LSTM(input_size=self.embedding_dim,\n",
    "                        hidden_size=self.hidden_dim, \n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=self.batch_first)\n",
    "\n",
    "    #dense layer / linear layer\n",
    "    self.fc = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
    "\n",
    "    #activation function\n",
    "    self.act = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, txt, txt_len):\n",
    "    print(\"txt\")\n",
    "    print(txt.shape)\n",
    "    print(txt)\n",
    "    print(txt_len.shape)\n",
    "    print(txt_len)\n",
    "    '''\n",
    "    # txt [batch_size, seq_len] \n",
    "    ~ seq_len is max sequence length among all the rows in batch\n",
    "    ~ it means the rows length with less than seq_len will be padded \n",
    "    ~ but the padding will be batchwise\n",
    "    # txt_len [batch_size]\n",
    "    ~ contains sequence length for each row in batch\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Step 1: pass through the embedding layer to convert text into vectors\n",
    "    '''\n",
    "    # embed_txt ~ [batch_size, seq_len, embedding_dim] \n",
    "    embed_txt = self.embedding(txt)\n",
    "    \n",
    "    # embed_txt = torch.sum(embed_txt, 1)\n",
    "    \n",
    "    print(\"embed_txt\")\n",
    "    print(embed_txt)\n",
    "    print(embed_txt.shape)\n",
    "    print(txt_len.shape)\n",
    "\n",
    "    '''\n",
    "    Step 2: passing the embeddings through LSTM layer\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    Step 2.1: first packing the embeddings to tackle variable length input\n",
    "    For pytorch to know how to pack and unpack properly, \n",
    "    we feed in the length of the original sentence (before padding).\n",
    "    by default enforce_sorted=True, \n",
    "    which requires input sorted by decreasing length, \n",
    "    just make sure the target y are also sorted accordingly. \n",
    "    '''\n",
    "    # packed the embedding (only the vocab words without padding)\n",
    "    embed_txt_packed_pad = nn.utils.rnn.pack_padded_sequence(embed_txt, txt_len.cpu(), batch_first=True)\n",
    "\n",
    "    \n",
    "    print(\"embed_txt_packed_pad\")\n",
    "    print(embed_txt)\n",
    "    \n",
    "    '''\n",
    "    Step 2.2: passing the packed input to LSTM layer\n",
    "    '''\n",
    "    # lstm_out ~ [batch_size, seq_len, (2 * hidden_dim)] \n",
    "    lstm_out, (h_n, c_n) = self.lstm(embed_txt_packed_pad)\n",
    "\n",
    "    '''\n",
    "    Step 2.3: retrieving back the lstm output with zero padding\n",
    "    '''\n",
    "    # packed the embedding (with padding)\n",
    "    embed_txt_pad_packed, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "    '''\n",
    "    Step 3: sum all the hidden states\n",
    "    '''\n",
    "    # lstm_out ~ [include dimention, remove dimention, include dimention] \n",
    "    # concat_out ~ [batch_size, (2 * hidden_dim)] #concatenate hidden states\n",
    "    # concat_out = embed_txt_pad_packed[ : , -1, : ]  #concatenate hidden states\n",
    "    sum_ip = embed_txt_pad_packed.sum(dim=1)  #summing up hidden states\n",
    "    # avg_ip = embed_txt_pad_packed.mean(dim=1)  #averaging the hidden states\n",
    "\n",
    "    '''\n",
    "    Step 4: feeding the weighted value to a linear layer\n",
    "    '''\n",
    "    # fc_out ~ [batch_size, output_dim]\n",
    "    fc_out = self.fc(sum_ip)\n",
    "\n",
    "    '''\n",
    "    Step 5: feeding the linear output to activation function\n",
    "    '''\n",
    "    # out ~ [batch_size, output_dim]\n",
    "    out = self.act(fc_out)\n",
    "\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
