{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8da09f3-fa36-4d17-82cd-8e87bd040c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting os environement\n",
    "import os\n",
    "os.environ[\"ENV_NM\"] = \"dev\"\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e929ce-a70d-49d5-bfde-7f446ff7836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from util.util_lib import *\n",
    "import util.util_cnst as cnst\n",
    "from CookingDataset import CookingDataset \n",
    "from CookingBatchSampler import CookingBatchSampler\n",
    "from CookingCollator import CookingCollator\n",
    "# from CookingGAN import CookingGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc0d319-b030-410d-ac32-21b5bf03c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config file\n",
    "def get_conf(): \n",
    "    conf = EnvYAML(cnst.CONFIG_FL)  \n",
    "    conf = conf[conf['env_nm']]\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89939711-e2a0-4aff-a2f2-a378ccefe5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(path, conf):\n",
    "    dl = torch.load(path)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1651ddf-4351-4a35-bbff-876e2560c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf, **kwargs):\n",
    "        #Constructor\n",
    "        super(Generator, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conf = conf\n",
    "        \n",
    "        self.batch_size = self.conf['data']['batch_size']\n",
    "        \n",
    "        #embedding layer\n",
    "        vocab_size = conf['vocab']['vocab_size']\n",
    "        self.embedding_dim = conf['vocab']['embed_size']\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                  embedding_dim=self.embedding_dim, \n",
    "                                     padding_idx = 0)\n",
    "        \n",
    "        vocab_wt_path = conf['vocab']['vocab_wt_path']\n",
    "        vocab_weight = torch.load(vocab_wt_path)\n",
    "        self.embedding = self.embedding.from_pretrained(vocab_weight)\n",
    "        # # kaiming_uniform_, uniform_\n",
    "        # torch.nn.init.uniform_(self.embedding.weight, 0, 1) # make sure embedding weights are b/w 0 and 1\n",
    "        # self.embedding.weight.data[0] = 0 # reset the index of padding_idx to zeros\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_1 = nn.Linear(self.embedding_dim, 256)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_4 = nn.Linear(64, 32)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.output_dim = conf['model']['info']['no_of_cls'] # number of classes\n",
    "        self.fc_5 = nn.Linear(32, self.output_dim)\n",
    "        \n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self, _txt):\n",
    "        \n",
    "        '''\n",
    "            Step 0: input parameters\n",
    "            # _txt ~ [batch_size, seq_len, word_len] \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "            Step 1: pass through the embedding layer to convert text into vectors\n",
    "            # embed_txt ~ [batch_size, seq_len, word_len, embedding_dim] \n",
    "        '''\n",
    "        _embed_txt = self.embedding(_txt)\n",
    "        \n",
    "        '''\n",
    "            Step 2: \n",
    "            # _embed_txt_sum_1 ~ [batch_size, seq_len, embedding_dim] \n",
    "            # _embed_txt_sum_2 ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _embed_txt_sum_1 = torch.sum(_embed_txt, 2)\n",
    "        _embed_txt_sum_2 = torch.sum(_updt_embed_txt_1, 1)\n",
    "        \n",
    "        '''\n",
    "           Step 3: \n",
    "           generating noise by random sampling from a normal distribution\n",
    "           # noise_ ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _noise = np.random.normal(0, 1, (self.batch_size, self.embedding_dim))\n",
    "        _noise = ((torch.from_numpy(_noise)).float())\n",
    "        \n",
    "        '''\n",
    "           Step 4:\n",
    "           sum noise to input text tensor\n",
    "           # _noisy_embed_txt ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _noisy_embed_txt = torch.add(_embed_txt_sum_2, _noise)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "           Step 5:\n",
    "           propagate through linear layer\n",
    "           # fc_out_1 ~ [batch_size, 256] \n",
    "           # fc_out_2 ~ [batch_size, 128] \n",
    "           # fc_out_3 ~ [batch_size, 64] \n",
    "           # fc_out_4 ~ [batch_size, 32] \n",
    "           # fc_out_5 ~ [batch_size, no_of_cls] \n",
    "        '''\n",
    "        fc_out_1 = self.fc_1(_noisy_embed_txt)\n",
    "        fc_out_2 = self.fc_2(fc_out_1)\n",
    "        fc_out_3 = self.fc_3(fc_out_2)\n",
    "        fc_out_4 = self.fc_4(fc_out_3)\n",
    "        fc_out_5 = self.fc_5(fc_out_4)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "            Step 6: feeding the linear output to activation function \n",
    "            # out ~ [batch_size, no_of_cls]\n",
    "        '''\n",
    "        out = self.act(fc_out_5)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94708a-4830-4257-b08b-0e10ac6d7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conf, **kwargs):\n",
    "        #Constructor\n",
    "        super(Discriminator, self).__init__(**kwargs)\n",
    "        \n",
    "        self.conf = conf\n",
    "        \n",
    "        self.batch_size = self.conf['data']['batch_size']\n",
    "        \n",
    "        #embedding layer\n",
    "        vocab_size = conf['vocab']['vocab_size']\n",
    "        self.embedding_dim = conf['vocab']['embed_size']\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                  embedding_dim=self.embedding_dim, \n",
    "                                     padding_idx = 0)\n",
    "        \n",
    "        vocab_wt_path = conf['vocab']['vocab_wt_path']\n",
    "        vocab_weight = torch.load(vocab_wt_path)\n",
    "        self.embedding = self.embedding.from_pretrained(vocab_weight)\n",
    "        # # kaiming_uniform_, uniform_\n",
    "        # torch.nn.init.uniform_(self.embedding.weight, 0, 1) # make sure embedding weights are b/w 0 and 1\n",
    "        # self.embedding.weight.data[0] = 0 # reset the index of padding_idx to zeros\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_1 = nn.Linear(self.embedding_dim, 256)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.fc_4 = nn.Linear(64, 32)\n",
    "        \n",
    "        #dense layer / linear layer\n",
    "        self.output_dim = conf['model']['info']['no_of_cls'] # number of classes\n",
    "        self.fc_5 = nn.Linear(32, self.output_dim)\n",
    "        \n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    def forward(self, _txt, _lbl):\n",
    "        \n",
    "        '''\n",
    "            Step 0: input parameters\n",
    "            # _txt ~ [batch_size, seq_len, word_len] \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "            Step 1: pass through the embedding layer to convert text into vectors\n",
    "            # embed_txt ~ [batch_size, seq_len, word_len, embedding_dim] \n",
    "        '''\n",
    "        _embed_txt = self.embedding(_txt)\n",
    "        \n",
    "        '''\n",
    "            Step 2: \n",
    "            # _embed_txt_sum_1 ~ [batch_size, seq_len, embedding_dim] \n",
    "            # _embed_txt_sum_2 ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _embed_txt_sum_1 = torch.sum(_embed_txt, 2)\n",
    "        _embed_txt_sum_2 = torch.sum(_updt_embed_txt_1, 1)\n",
    "        \n",
    "        '''\n",
    "           Step 3: \n",
    "           generating noise by random sampling from a normal distribution\n",
    "           # noise_ ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _noise = np.random.normal(0, 1, (self.batch_size, self.embedding_dim))\n",
    "        _noise = ((torch.from_numpy(_noise)).float())\n",
    "        \n",
    "        '''\n",
    "           Step 4:\n",
    "           sum noise to input text tensor\n",
    "           # _noisy_embed_txt ~ [batch_size, embedding_dim] \n",
    "        '''\n",
    "        _noisy_embed_txt = torch.add(_embed_txt_sum_2, _noise)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "           Step 5:\n",
    "           propagate through linear layer\n",
    "           # fc_out_1 ~ [batch_size, 256] \n",
    "           # fc_out_2 ~ [batch_size, 128] \n",
    "           # fc_out_3 ~ [batch_size, 64] \n",
    "           # fc_out_4 ~ [batch_size, 32] \n",
    "           # fc_out_5 ~ [batch_size, no_of_cls] \n",
    "        '''\n",
    "        fc_out_1 = self.fc_1(_noisy_embed_txt)\n",
    "        fc_out_2 = self.fc_2(fc_out_1)\n",
    "        fc_out_3 = self.fc_3(fc_out_2)\n",
    "        fc_out_4 = self.fc_4(fc_out_3)\n",
    "        fc_out_5 = self.fc_5(fc_out_4)\n",
    "        \n",
    "        '''\n",
    "            Step 6:\n",
    "            converting lbl to one hot vectors\n",
    "            # _lbl_one_hot_vec ~ [batch_size, no_of_cls] \n",
    "        '''\n",
    "        _lbl_one_hot_vec = torch.nn.functional.one_hot(_lbl)\n",
    "        \n",
    "         '''\n",
    "            Step 7:\n",
    "            combining _lbl and _txt\n",
    "            # _lbl_one_hot_vec ~ [batch_size, no_of_cls] \n",
    "        '''       \n",
    "        fc_out = torch.add(fc_out_5, _lbl)\n",
    "        \n",
    "        '''\n",
    "            Step 8: feeding the linear output to activation function \n",
    "            # out ~ [batch_size, no_of_cls]\n",
    "        '''\n",
    "        out = self.act(fc_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b9e4d9-f299-4ebb-8c13-777a30426935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CookingGAN:\n",
    "    def __init__(self, conf):\n",
    "        \n",
    "        \n",
    "        self.conf = conf\n",
    "        \n",
    "        self.N_EPOCHS = conf['model']['info']['train_epoch']\n",
    "        self.VALIDATION_EPOCH = conf['model']['info']['valid_epoch']\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        #instantiate the model\n",
    "        self.discriminator = Discriminator(conf)\n",
    "        self.discriminator = self.discriminator.to(device)\n",
    "        \n",
    "        self.generator = Generator(conf)\n",
    "        self.generator = self.generator.to(device)\n",
    "        \n",
    "        #define the optimizer\n",
    "        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002)\n",
    "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002)\n",
    "        \n",
    "        \n",
    "        #define the loss\n",
    "        num_cls = conf['model']['info']['no_of_cls']\n",
    "        self.criterion = MulticlassHingeLoss(num_classes=num_cls) # Multiclass Hinge loss, Entropy loss\n",
    "        self.criterion = self.criterion.to(device)\n",
    "    \n",
    "    #define metric\n",
    "    def multicls_accuracy(self, preds, y):\n",
    "        acc = multiclass_accuracy(preds, y)\n",
    "        return acc\n",
    "\n",
    "    def valid_model(self, valid_iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for valid_batch in valid_iterator:\n",
    "\n",
    "                #retrieve text\n",
    "                _text, _lbl = valid_batch['text_to_vocab_list'], valid_batch['label_to_encode_list']\n",
    "\n",
    "                #get prediction\n",
    "                predictions = self.model(_text)\n",
    "                preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "\n",
    "                #compute the loss\n",
    "                loss = self.criterion(preds, _lbl)\n",
    "\n",
    "                #compute the binary accuracy\n",
    "                acc = self.multicls_accuracy(preds, _lbl)\n",
    "\n",
    "                # compute loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        valid_epoc_loss = epoch_loss / len(valid_iterator)\n",
    "        valid_epoch_acc = epoch_acc / len(valid_iterator)\n",
    "\n",
    "        return valid_epoc_loss, valid_epoch_acc\n",
    "    \n",
    "    def train(self, train_dl, valid_dl):\n",
    "        \n",
    "        #set the model in training phase\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(self.N_EPOCHS+1):\n",
    "            \n",
    "            #initialize every epoch \n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            \n",
    "            for train_batch in train_dl:\n",
    "                #resets the gradients after every batch\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                _text, _lbl = train_batch['text_to_vocab_list'], train_batch['label_to_encode_list']\n",
    "                \n",
    "                _predictions = self.model(_text)\n",
    "                _preds = _predictions.squeeze(-1) #convert to 1D tensor\n",
    "                \n",
    "                #compute the loss\n",
    "                loss = self.criterion(_preds, _lbl)\n",
    "                \n",
    "                #compute the binary accuracy\n",
    "                acc = self.multicls_accuracy(_preds, _lbl)\n",
    "                \n",
    "                #backpropage the loss and compute the gradients\n",
    "                loss.backward()\n",
    "                \n",
    "                #update the weights\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "                \n",
    "            if epoch%self.VALIDATION_EPOCH == 0:\n",
    "                self.model.eval() # set the model in eval phase\n",
    "                valid_epoc_loss, valid_epoch_acc = self.valid_model(valid_dl)\n",
    "                self.model.train() # return back to training phase\n",
    "\n",
    "                print(f\"epoch:- \",epoch)\n",
    "                print(f\"training===> \",\"loss:- \", epoch_loss / len(train_dl), \"  accuracy:- \", epoch_acc / len(train_dl))\n",
    "                print(f\"validation===> \",\"loss:- \", valid_epoc_loss, \"  accuracy:- \", valid_epoch_acc)\n",
    "\n",
    "            if epoch == self.N_EPOCHS:\n",
    "                g_path = self.conf['data']['data_fl_path'] + self.conf['model']['model_path']\n",
    "                torch.save(self.model.state_dict(), g_path)\n",
    "                print(f\"model saved:- {g_path}\")\n",
    "                \n",
    "                \n",
    "    def test(self, test_iterator):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_batch in test_iterator:\n",
    "\n",
    "                #retrieve text\n",
    "                _text = test_batch['text_to_vocab_list']\n",
    "\n",
    "                #get prediction\n",
    "                predictions = self.model(_text)\n",
    "                preds = predictions.squeeze(-1) #convert to 1D tensor\n",
    "                \n",
    "                # receive output logits\n",
    "                _, preds = torch.max(preds, 1)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e64b4f-0b8e-4b88-bd16-f541e73c9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, valid_dl, conf):\n",
    "    \n",
    "    # GAN model train and validation\n",
    "    cooking_model = CookingGAN(conf)\n",
    "    cooking_model.train(train_dl, valid_dl)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7870ad-e46b-4af4-8734-ece8bfbe4147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:-  0\n",
      "training===>  loss:-  0.12556393488606102   accuracy:-  0.001974711126788002\n",
      "validation===>  loss:-  0.12508833480285894   accuracy:-  0.0018856065367693275\n",
      "epoch:-  2\n",
      "training===>  loss:-  0.12501964012102418   accuracy:-  0.001770430665428437\n",
      "validation===>  loss:-  0.12508188117457064   accuracy:-  0.0017127592708988059\n",
      "epoch:-  4\n",
      "training===>  loss:-  0.12501964074294006   accuracy:-  0.0015740071448903938\n",
      "validation===>  loss:-  0.1250808235012009   accuracy:-  0.0015556253928346953\n",
      "epoch:-  6\n",
      "training===>  loss:-  0.1250196618543624   accuracy:-  0.0015465078520150678\n",
      "validation===>  loss:-  0.1250814626982645   accuracy:-  0.0014613450659962288\n",
      "model saved:- data/model/whats_cooking_model.pt\n",
      "Model Training is FINISHED\n",
      "CPU times: user 13min 3s, sys: 12.1 s, total: 13min 15s\n",
      "Wall time: 14min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # reading configuration\n",
    "        conf = get_conf()\n",
    "        \n",
    "        # load data loaders\n",
    "        dataloader_path = conf['data']['data_fl_path'] + conf['data']['train_dataloader']\n",
    "        train_dl = get_dataloader(dataloader_path, conf)\n",
    "        dataloader_path = conf['data']['data_fl_path'] + conf['data']['valid_dataloader']\n",
    "        valid_dl = get_dataloader(dataloader_path, conf)\n",
    "        \n",
    "        # train the model\n",
    "        train_model(train_dl, valid_dl, conf)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "        print(\"Model Training is FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9aa3d-9917-4ddf-8ada-f58906bdc5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4259345-3d9d-489e-ad29-67c2cb8b01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hussainwali.medium.com/using-fasttext-embeddings-in-pytorch-boosting-neural-network-performance-fe017c39c7c3\n",
    "# https://hussainwali.medium.com/transforming-your-text-data-with-pytorch-12ec1b1c9ae6\n",
    "# https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb\n",
    "# https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/\n",
    "# https://github.com/eriklindernoren/PyTorch-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd578a-436f-4c71-82fb-23c0a54f821d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
